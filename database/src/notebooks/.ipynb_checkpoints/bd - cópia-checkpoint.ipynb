{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2504067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "           PIPELINE BANCO SIH-RS NORMALIZADO\n",
      "                   VERSÃO FINAL\n",
      "======================================================================\n",
      "INFO: INICIANDO PIPELINE COMPLETO\n",
      "INFO: Data/hora: 2025-06-05 00:54:54.909425\n",
      "Memória: 34.3% usada (10.5GB disponível)\n",
      "INFO: ETAPA 1: CONEXÃO E VALIDAÇÕES\n",
      "INFO: Conectando ao PostgreSQL...\n",
      "INFO: Conectado ao PostgreSQL com sucesso!\n",
      "INFO: Arquivo principal: 353.6 MB\n",
      "Memória: 34.4% usada (10.5GB disponível)\n",
      "INFO: Validações concluídas\n",
      "INFO: ETAPA 2: CARREGAMENTO DE DADOS\n",
      "INFO: Carregando dados principais...\n",
      "INFO: Carregados 11,022,199 registros\n",
      "INFO: AIHs únicas: 11,022,199\n",
      "INFO: Nenhuma duplicata encontrada\n",
      "Memória: 70.1% usada (4.8GB disponível)\n",
      "INFO: ETAPA 3: CRIAÇÃO DE TABELAS NORMALIZADAS\n",
      "INFO: Criando tabelas normalizadas...\n",
      "INFO: Processando tabela 'internacoes'...\n",
      "INFO: Colunas incluídas: 54\n",
      "INFO: Registros válidos: 11,022,199\n",
      "INFO:     Processados 10/221 chunks\n",
      "\n",
      "Execução finalizada em 2025-06-05 01:02:36.452684\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 955\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m         sucesso \u001b[38;5;241m=\u001b[39m \u001b[43mexecutar_pipeline_completo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sucesso:\n\u001b[1;32m    957\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEXECUÇÃO CONCLUÍDA COM SUCESSO!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 889\u001b[0m, in \u001b[0;36mexecutar_pipeline_completo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# 3. Criar tabelas normalizadas\u001b[39;00m\n\u001b[1;32m    888\u001b[0m log_operacao(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETAPA 3: CRIAÇÃO DE TABELAS NORMALIZADAS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m stats_principais \u001b[38;5;241m=\u001b[39m \u001b[43mcriar_tabelas_normalizadas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m df\n\u001b[1;32m    892\u001b[0m limpar_memoria()\n",
      "Cell \u001b[0;32mIn[1], line 177\u001b[0m, in \u001b[0;36mcriar_tabelas_normalizadas\u001b[0;34m(engine, df)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_tabela), CHUNKSIZE):\n\u001b[1;32m    175\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m df_tabela\u001b[38;5;241m.\u001b[39miloc[i:i\u001b[38;5;241m+\u001b[39mCHUNKSIZE]\n\u001b[0;32m--> 177\u001b[0m     \u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnome_tabela\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m                \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulti\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# Log apenas a cada 10 chunks para reduzir output\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_chunks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (i\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mCHUNKSIZE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/pandas/core/generic.py:3087\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[1;32m   2891\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;124;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   3085\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[0;32m-> 3087\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/pandas/io/sql.py:842\u001b[0m, in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument should be either a Series or a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    839\u001b[0m     )\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema, need_transaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/pandas/io/sql.py:2018\u001b[0m, in \u001b[0;36mSQLDatabase.to_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m   2006\u001b[0m sql_engine \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m   2008\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_table(\n\u001b[1;32m   2009\u001b[0m     frame\u001b[38;5;241m=\u001b[39mframe,\n\u001b[1;32m   2010\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2015\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   2016\u001b[0m )\n\u001b[0;32m-> 2018\u001b[0m total_inserted \u001b[38;5;241m=\u001b[39m \u001b[43msql_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_records\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_case_sensitive(name\u001b[38;5;241m=\u001b[39mname, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_inserted\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/pandas/io/sql.py:1558\u001b[0m, in \u001b[0;36mSQLAlchemyEngine.insert_records\u001b[0;34m(self, table, con, frame, name, index, schema, chunksize, method, **engine_kwargs)\u001b[0m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exc\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mStatementError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1560\u001b[0m     \u001b[38;5;66;03m# GH34431\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m     \u001b[38;5;66;03m# https://stackoverflow.com/a/67358288/6067848\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(1054, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown column \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf(e0)?\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield list\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m))(?#\u001b[39m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;124m    )|inf can not be used with MySQL\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/pandas/io/sql.py:1119\u001b[0m, in \u001b[0;36mSQLTable.insert\u001b[0;34m(self, chunksize, method)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m chunk_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(arr[start_i:end_i] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m data_list))\n\u001b[0;32m-> 1119\u001b[0m num_inserted \u001b[38;5;241m=\u001b[39m \u001b[43mexec_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# GH 46891\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_inserted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/pandas/io/sql.py:1026\u001b[0m, in \u001b[0;36mSQLTable._execute_insert_multi\u001b[0;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m insert\n\u001b[1;32m   1025\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(keys, row)) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data_iter]\n\u001b[0;32m-> 1026\u001b[0m stmt \u001b[38;5;241m=\u001b[39m \u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m result \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mexecute(stmt)\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mrowcount\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mvalues\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/sqlalchemy/sql/base.py:280\u001b[0m, in \u001b[0;36m_generative.<locals>._generative\u001b[0;34m(fn, self, *args, **kw)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mark a method as generative.\"\"\"\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate()\n\u001b[0;32m--> 280\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerative methods must return self\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mvalues\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/sqlalchemy/sql/base.py:313\u001b[0m, in \u001b[0;36m_exclusive_against.<locals>.check\u001b[0;34m(fn, *args, **kw)\u001b[0m\n\u001b[1;32m    307\u001b[0m         msg \u001b[38;5;241m=\u001b[39m msgs\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m             name,\n\u001b[1;32m    309\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() has already been invoked on this \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m construct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m             \u001b[38;5;241m%\u001b[39m (fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m),\n\u001b[1;32m    311\u001b[0m         )\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mInvalidRequestError(msg)\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/sqlalchemy/sql/dml.py:1149\u001b[0m, in \u001b[0;36mValuesBase.values\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1146\u001b[0m     multi_kv_generator \u001b[38;5;241m=\u001b[39m DMLState\u001b[38;5;241m.\u001b[39mget_plugin_class(\n\u001b[1;32m   1147\u001b[0m         \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1148\u001b[0m     )\u001b[38;5;241m.\u001b[39m_get_multi_crud_kv_pairs\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_values \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mmulti_kv_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m,)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/sqlalchemy/sql/dml.py:175\u001b[0m, in \u001b[0;36mDMLState._get_multi_crud_kv_pairs\u001b[0;34m(cls, statement, multi_kv_iterator)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_multi_crud_kv_pairs\u001b[39m(\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    172\u001b[0m     statement: UpdateBase,\n\u001b[1;32m    173\u001b[0m     multi_kv_iterator: Iterable[Dict[_DMLColumnArgument, Any]],\n\u001b[1;32m    174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[_DMLColumnElement, Any]]:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    176\u001b[0m         {\n\u001b[1;32m    177\u001b[0m             coercions\u001b[38;5;241m.\u001b[39mexpect(roles\u001b[38;5;241m.\u001b[39mDMLColumnRole, k): v\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    179\u001b[0m         }\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m multi_kv_iterator\n\u001b[1;32m    181\u001b[0m     ]\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/sqlalchemy/sql/dml.py:176\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_multi_crud_kv_pairs\u001b[39m(\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    172\u001b[0m     statement: UpdateBase,\n\u001b[1;32m    173\u001b[0m     multi_kv_iterator: Iterable[Dict[_DMLColumnArgument, Any]],\n\u001b[1;32m    174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[_DMLColumnElement, Any]]:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 176\u001b[0m         {\n\u001b[1;32m    177\u001b[0m             coercions\u001b[38;5;241m.\u001b[39mexpect(roles\u001b[38;5;241m.\u001b[39mDMLColumnRole, k): v\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    179\u001b[0m         }\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m multi_kv_iterator\n\u001b[1;32m    181\u001b[0m     ]\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/sqlalchemy/sql/dml.py:177\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_multi_crud_kv_pairs\u001b[39m(\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    172\u001b[0m     statement: UpdateBase,\n\u001b[1;32m    173\u001b[0m     multi_kv_iterator: Iterable[Dict[_DMLColumnArgument, Any]],\n\u001b[1;32m    174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[_DMLColumnElement, Any]]:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    176\u001b[0m         {\n\u001b[0;32m--> 177\u001b[0m             \u001b[43mcoercions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDMLColumnRole\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m: v\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    179\u001b[0m         }\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m multi_kv_iterator\n\u001b[1;32m    181\u001b[0m     ]\n",
      "File \u001b[0;32m~/Faculdade/IC/projeto-ic-datasus/venv310/lib/python3.10/site-packages/sqlalchemy/sql/coercions.py:375\u001b[0m, in \u001b[0;36mexpect\u001b[0;34m(role, element, apply_propagate_attrs, argname, post_inspect, disable_inspection, **kw)\u001b[0m\n\u001b[1;32m    373\u001b[0m     is_clause_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__clause_element__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    376\u001b[0m         is_clause_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(element, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_clause_element\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BANCO DE DADOS SIH-RS - ESTRUTURA NORMALIZADA\n",
    "# Implementa todas as modificações solicitadas conforme reunião\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PARQUET_PATH = \"../../banco/parquet_unificado/sih_rs_tratado.parquet\"\n",
    "CHUNKSIZE = 50000\n",
    "\n",
    "DB_CONFIG = {\n",
    "    'usuario': 'postgres',\n",
    "    'senha': '1234',\n",
    "    'host': 'localhost',\n",
    "    'porta': '5432',\n",
    "    'banco': 'sih_rs'\n",
    "}\n",
    "\n",
    "CSVS_APOIO = {\n",
    "    'procedimentos': '../../procedimentos.csv',\n",
    "    'municipios': '../../municipios_cod.csv',\n",
    "    'cid10': '../../cid10.csv'\n",
    "}\n",
    "\n",
    "# Estrutura das tabelas conforme diagrama do banco\n",
    "ESTRUTURA_TABELAS = {\n",
    "    'internacoes': {\n",
    "        'pk': 'N_AIH',\n",
    "        'colunas': [\n",
    "            'N_AIH', 'IDENT', 'CEP', 'MUNIC_RES', 'NASC', 'SEXO', \n",
    "            'DT_INTER', 'DT_SAIDA', 'PROC_SOLIC', 'PROC_REA', \n",
    "            'NATUREZA', 'CNES', 'NAT_JUR', 'GESTAO', 'IND_VDRL', \n",
    "            'IDADE', 'DIAG_PRINC', 'DIAG_SECUN', 'COBRANCA', 'MORTE',\n",
    "            'MUNIC_MOV', 'DIAS_PERM', 'NACIONAL', 'NUM_FILHOS', 'INSTRU',\n",
    "            'CID_NOTIF', 'CBOR', 'CNAER', 'VINCPREV', 'INFEHOSP',\n",
    "            'CID_ASSO', 'CID_MORTE', 'COMPLEX', 'RACA_COR', 'ETNIA',\n",
    "            'DIAGSEC1', 'DIAGSEC2', 'DIAGSEC3', 'DIAGSEC4', 'DIAGSEC5',\n",
    "            'DIAGSEC6', 'DIAGSEC7', 'DIAGSEC8', 'DIAGSEC9',\n",
    "            'TPDISEC1', 'TPDISEC2', 'TPDISEC3', 'TPDISEC4', 'TPDISEC5',\n",
    "            'TPDISEC6', 'TPDISEC7', 'TPDISEC8', 'TPDISEC9', 'ESPEC'\n",
    "        ]\n",
    "    },\n",
    "    'financeiro': {\n",
    "        'pk': 'N_AIH',\n",
    "        'fk': [('N_AIH', 'internacoes')],\n",
    "        'colunas': ['N_AIH', 'VAL_UTI', 'VAL_SP', 'VAL_SH', 'VAL_TOT']\n",
    "    },\n",
    "    'uti_info': {\n",
    "        'pk': 'N_AIH',\n",
    "        'fk': [('N_AIH', 'internacoes')],\n",
    "        'colunas': ['N_AIH', 'UTI_MES_TO', 'MARCA_UTI', 'UTI_INT_TO', 'DIAR_ACOM']\n",
    "    },\n",
    "    'obstetricos': {\n",
    "        'pk': 'N_AIH',\n",
    "        'fk': [('N_AIH', 'internacoes')],\n",
    "        'colunas': ['N_AIH', 'GESTRICO', 'INSC_PN', 'CONTRACEP1', 'CONTRACEP2']\n",
    "    }\n",
    "}\n",
    "\n",
    "def verificar_memoria():\n",
    "    \"\"\"Monitora uso de memória para evitar crashes em notebooks\"\"\"\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Memória: {memory.percent:.1f}% usada ({memory.available / (1024**3):.1f}GB disponível)\")\n",
    "    return memory.percent\n",
    "\n",
    "def limpar_memoria():\n",
    "    gc.collect()\n",
    "\n",
    "def log_operacao(mensagem, nivel=\"INFO\"):\n",
    "    print(f\"{nivel}: {mensagem}\")\n",
    "\n",
    "def conectar_validar():\n",
    "    \"\"\"Conecta ao PostgreSQL e valida arquivos necessários\"\"\"\n",
    "    log_operacao(\"Conectando ao PostgreSQL...\")\n",
    "    \n",
    "    connection_string = f\"postgresql+psycopg2://{DB_CONFIG['usuario']}:{DB_CONFIG['senha']}@{DB_CONFIG['host']}:{DB_CONFIG['porta']}/{DB_CONFIG['banco']}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    try:\n",
    "        test = pd.read_sql(\"SELECT 1 as teste\", engine)\n",
    "        log_operacao(\"Conectado ao PostgreSQL com sucesso!\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Erro de conexão: {e}\")\n",
    "    \n",
    "    if not os.path.exists(PARQUET_PATH):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado: {PARQUET_PATH}\")\n",
    "    \n",
    "    tamanho_mb = os.path.getsize(PARQUET_PATH) / (1024 * 1024)\n",
    "    log_operacao(f\"Arquivo principal: {tamanho_mb:.1f} MB\")\n",
    "    \n",
    "    verificar_memoria()\n",
    "    log_operacao(\"Validações concluídas\")\n",
    "    return engine\n",
    "\n",
    "def carregar_dados_principais():\n",
    "    \"\"\"Carrega dados tratados e verifica integridade N_AIH\"\"\"\n",
    "    log_operacao(\"Carregando dados principais...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_parquet(PARQUET_PATH, engine=\"pyarrow\")\n",
    "        log_operacao(f\"Carregados {len(df):,} registros\")\n",
    "        log_operacao(f\"AIHs únicas: {df['N_AIH'].nunique():,}\")\n",
    "        \n",
    "        duplicatas = len(df) - df['N_AIH'].nunique()\n",
    "        if duplicatas > 0:\n",
    "            log_operacao(f\"ATENÇÃO: {duplicatas:,} registros duplicados por N_AIH!\", \"WARNING\")\n",
    "            \n",
    "            dups = df[df.duplicated('N_AIH', keep=False)].sort_values('N_AIH')\n",
    "            if len(dups) > 0:\n",
    "                log_operacao(\"Exemplo de duplicatas encontradas:\")\n",
    "                exemplo = dups[['N_AIH', 'DT_INTER', 'PROC_REA']].head(6)\n",
    "                print(exemplo.to_string(index=False))\n",
    "        else:\n",
    "            log_operacao(\"Nenhuma duplicata encontrada\")\n",
    "        \n",
    "        nulos_naih = df['N_AIH'].isnull().sum()\n",
    "        if nulos_naih > 0:\n",
    "            log_operacao(f\"{nulos_naih:,} registros com N_AIH nulo\", \"WARNING\")\n",
    "        \n",
    "        verificar_memoria()\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_operacao(f\"Erro ao carregar dados: {e}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "def criar_tabelas_normalizadas(engine, df):\n",
    "    \"\"\"Cria estrutura normalizada conforme diagrama do banco\"\"\"\n",
    "    log_operacao(\"Criando tabelas normalizadas...\")\n",
    "    \n",
    "    estatisticas = {}\n",
    "    \n",
    "    for nome_tabela, config in ESTRUTURA_TABELAS.items():\n",
    "        log_operacao(f\"Processando tabela '{nome_tabela}'...\")\n",
    "        \n",
    "        colunas_existentes = [col for col in config['colunas'] if col in df.columns]\n",
    "        colunas_faltantes = [col for col in config['colunas'] if col not in df.columns]\n",
    "        \n",
    "        # Log apenas se houver colunas faltantes relevantes\n",
    "        if colunas_faltantes:\n",
    "            log_operacao(f\"Colunas não encontradas: {colunas_faltantes}\", \"WARNING\")\n",
    "        \n",
    "        if not colunas_existentes or 'N_AIH' not in colunas_existentes:\n",
    "            log_operacao(f\"Erro: tabela {nome_tabela} não pode ser criada\", \"ERROR\")\n",
    "            continue\n",
    "        \n",
    "        df_tabela = df[colunas_existentes].copy()\n",
    "        \n",
    "        # Remove registros com N_AIH nulo \n",
    "        antes_limpeza = len(df_tabela)\n",
    "        df_tabela = df_tabela.dropna(subset=['N_AIH'])\n",
    "        apos_limpeza = len(df_tabela)\n",
    "        \n",
    "        if antes_limpeza != apos_limpeza:\n",
    "            removidos = antes_limpeza - apos_limpeza\n",
    "            log_operacao(f\"Removidos {removidos:,} registros com N_AIH nulo\")\n",
    "        \n",
    "        log_operacao(f\"Colunas incluídas: {len(colunas_existentes)}\")\n",
    "        log_operacao(f\"Registros válidos: {len(df_tabela):,}\")\n",
    "        \n",
    "        # Inserção em chunks --> otimizar memória\n",
    "        total_chunks = (len(df_tabela) // CHUNKSIZE) + 1\n",
    "        \n",
    "        try:\n",
    "            for i in range(0, len(df_tabela), CHUNKSIZE):\n",
    "                chunk = df_tabela.iloc[i:i+CHUNKSIZE]\n",
    "                \n",
    "                chunk.to_sql(nome_tabela, engine, \n",
    "                            if_exists=\"append\" if i > 0 else \"replace\", \n",
    "                            index=False, method='multi')\n",
    "                \n",
    "                if total_chunks > 10 and (i//CHUNKSIZE + 1) % 10 == 0:\n",
    "                    chunk_num = i//CHUNKSIZE + 1\n",
    "                    log_operacao(f\"    Processados {chunk_num}/{total_chunks} chunks\")\n",
    "            \n",
    "            count_result = pd.read_sql(f\"SELECT COUNT(*) as total FROM {nome_tabela}\", engine)\n",
    "            registros_inseridos = count_result['total'].iloc[0]\n",
    "            \n",
    "            log_operacao(f\"Tabela '{nome_tabela}' criada: {registros_inseridos:,} registros\")\n",
    "            \n",
    "            estatisticas[nome_tabela] = {\n",
    "                'colunas': len(colunas_existentes),\n",
    "                'registros': registros_inseridos,\n",
    "                'colunas_faltantes': len(colunas_faltantes)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_operacao(f\"Erro ao criar tabela {nome_tabela}: {e}\", \"ERROR\")\n",
    "            continue\n",
    "        \n",
    "        del df_tabela\n",
    "        limpar_memoria()\n",
    "    \n",
    "    log_operacao(\"Resumo das tabelas criadas:\")\n",
    "    for tabela, stats in estatisticas.items():\n",
    "        log_operacao(f\"   {tabela:12}: {stats['registros']:>8,} registros, {stats['colunas']:>2} colunas\")\n",
    "    \n",
    "    return estatisticas\n",
    "\n",
    "def carregar_tabelas_apoio(engine):\n",
    "    \"\"\"Carrega CSVs com detecção automática de formato\"\"\"\n",
    "    log_operacao(\"Carregando tabelas de apoio...\")\n",
    "    \n",
    "    separadores = [';', ',', '\\t']\n",
    "    encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']\n",
    "    \n",
    "    estatisticas_apoio = {}\n",
    "    \n",
    "    for nome_tabela, csv_path in CSVS_APOIO.items():\n",
    "        log_operacao(f\"Processando '{nome_tabela}'...\")\n",
    "        \n",
    "        if not os.path.exists(csv_path):\n",
    "            log_operacao(f\"Arquivo não encontrado: {csv_path}\", \"WARNING\")\n",
    "            continue\n",
    "        \n",
    "        tamanho_kb = os.path.getsize(csv_path) / 1024\n",
    "        log_operacao(f\"Tamanho: {tamanho_kb:.1f} KB\")\n",
    "        \n",
    "        df_csv = None\n",
    "        config_sucesso = None\n",
    "        \n",
    "        for sep in separadores:\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    df_test = pd.read_csv(csv_path, sep=sep, encoding=encoding, nrows=10)\n",
    "                    \n",
    "                    if len(df_test.columns) > 1:\n",
    "                        df_csv = pd.read_csv(csv_path, sep=sep, encoding=encoding)\n",
    "                        config_sucesso = f\"sep='{sep}', encoding='{encoding}'\"\n",
    "                        break\n",
    "                        \n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            if df_csv is not None:\n",
    "                break\n",
    "        \n",
    "        if df_csv is None:\n",
    "            log_operacao(f\"Não foi possível processar {csv_path}\", \"ERROR\")\n",
    "            continue\n",
    "        \n",
    "        log_operacao(f\"Carregado com {config_sucesso}\")\n",
    "        log_operacao(f\"Registros: {len(df_csv):,}, Colunas: {len(df_csv.columns)}\")\n",
    "        \n",
    "        df_csv.columns = df_csv.columns.str.strip()\n",
    "        colunas_originais = list(df_csv.columns)\n",
    "        \n",
    "        if nome_tabela == 'procedimentos':\n",
    "            mapeamento_proc = {\n",
    "                'codigo': 'PROC_REA',\n",
    "                'cod': 'PROC_REA', \n",
    "                'procedure': 'PROC_REA',\n",
    "                'nome': 'NOME_PROC',\n",
    "                'descricao': 'NOME_PROC',\n",
    "                'description': 'NOME_PROC'\n",
    "            }\n",
    "            \n",
    "            for col_original in df_csv.columns:\n",
    "                for key, new_name in mapeamento_proc.items():\n",
    "                    if key.lower() in col_original.lower():\n",
    "                        df_csv = df_csv.rename(columns={col_original: new_name})\n",
    "                        break\n",
    "            \n",
    "            if 'PROC_REA' not in df_csv.columns:\n",
    "                log_operacao(\"Coluna PROC_REA não encontrada em procedimentos\", \"WARNING\")\n",
    "            if 'NOME_PROC' not in df_csv.columns:\n",
    "                log_operacao(\"Coluna NOME_PROC não encontrada em procedimentos\", \"WARNING\")\n",
    "                \n",
    "        elif nome_tabela == 'municipios':\n",
    "            mapeamento_mun = {\n",
    "                'codigo': 'codigo_municipio_6d',\n",
    "                'cod_municipio': 'codigo_municipio_6d',\n",
    "                'municipio': 'nome',\n",
    "                'nome_municipio': 'nome',\n",
    "                'uf': 'UF',\n",
    "                'estado': 'UF',\n",
    "                'lat': 'latitude',\n",
    "                'latitude': 'latitude',\n",
    "                'lon': 'longitude',\n",
    "                'longitude': 'longitude'\n",
    "            }\n",
    "            \n",
    "            for col_original in df_csv.columns:\n",
    "                for key, new_name in mapeamento_mun.items():\n",
    "                    if key.lower() in col_original.lower():\n",
    "                        df_csv = df_csv.rename(columns={col_original: new_name})\n",
    "                        break\n",
    "        \n",
    "        elif nome_tabela == 'cid10':\n",
    "            mapeamento_cid = {\n",
    "                'codigo': 'CID_COD',\n",
    "                'cod': 'CID_COD',\n",
    "                'cid': 'CID_COD',\n",
    "                'descricao': 'CID_NOME',\n",
    "                'nome': 'CID_NOME',\n",
    "                'description': 'CID_NOME'\n",
    "            }\n",
    "            \n",
    "            for col_original in df_csv.columns:\n",
    "                for key, new_name in mapeamento_cid.items():\n",
    "                    if key.lower() in col_original.lower():\n",
    "                        df_csv = df_csv.rename(columns={col_original: new_name})\n",
    "                        break\n",
    "        \n",
    "        # Remove duplicatas em chaves primárias\n",
    "        if nome_tabela == 'procedimentos' and 'PROC_REA' in df_csv.columns:\n",
    "            antes_dup = len(df_csv)\n",
    "            df_csv = df_csv.drop_duplicates('PROC_REA')\n",
    "            apos_dup = len(df_csv)\n",
    "            if antes_dup != apos_dup:\n",
    "                log_operacao(f\"Removidas {antes_dup - apos_dup} duplicatas em PROC_REA\")\n",
    "        \n",
    "        elif nome_tabela == 'municipios' and 'codigo_municipio_6d' in df_csv.columns:\n",
    "            antes_dup = len(df_csv)\n",
    "            df_csv = df_csv.drop_duplicates('codigo_municipio_6d')\n",
    "            apos_dup = len(df_csv)\n",
    "            if antes_dup != apos_dup:\n",
    "                log_operacao(f\"Removidas {antes_dup - apos_dup} duplicatas em codigo_municipio_6d\")\n",
    "        \n",
    "        try:\n",
    "            df_csv.to_sql(nome_tabela, engine, if_exists='replace', index=False)\n",
    "            \n",
    "            count_result = pd.read_sql(f\"SELECT COUNT(*) as total FROM {nome_tabela}\", engine)\n",
    "            registros_inseridos = count_result['total'].iloc[0]\n",
    "            \n",
    "            log_operacao(f\"{registros_inseridos:,} registros inseridos\")\n",
    "            log_operacao(f\"Colunas finais: {list(df_csv.columns)}\")\n",
    "            \n",
    "            estatisticas_apoio[nome_tabela] = {\n",
    "                'registros': registros_inseridos,\n",
    "                'colunas_originais': colunas_originais,\n",
    "                'colunas_finais': list(df_csv.columns)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_operacao(f\"Erro ao inserir {nome_tabela}: {e}\", \"ERROR\")\n",
    "    \n",
    "    log_operacao(\"Resumo das tabelas de apoio:\")\n",
    "    for tabela, stats in estatisticas_apoio.items():\n",
    "        log_operacao(f\"   {tabela:12}: {stats['registros']:>6,} registros\")\n",
    "    \n",
    "    return estatisticas_apoio\n",
    "\n",
    "def criar_chaves_relacionamentos(engine):\n",
    "    \"\"\"Implementa chaves primárias e estrangeiras conforme diagrama\"\"\"\n",
    "    log_operacao(\"Criando chaves e relacionamentos...\")\n",
    "    \n",
    "    comandos_sql = [\n",
    "        # Chaves primárias\n",
    "        {\n",
    "            'sql': \"ALTER TABLE internacoes ADD CONSTRAINT pk_internacoes PRIMARY KEY (N_AIH);\",\n",
    "            'desc': \"PK internacoes\",\n",
    "            'tipo': \"PK\"\n",
    "        },\n",
    "        {\n",
    "            'sql': \"ALTER TABLE financeiro ADD CONSTRAINT pk_financeiro PRIMARY KEY (N_AIH);\",\n",
    "            'desc': \"PK financeiro\", \n",
    "            'tipo': \"PK\"\n",
    "        },\n",
    "        {\n",
    "            'sql': \"ALTER TABLE uti_info ADD CONSTRAINT pk_uti_info PRIMARY KEY (N_AIH);\",\n",
    "            'desc': \"PK uti_info\",\n",
    "            'tipo': \"PK\"\n",
    "        },\n",
    "        {\n",
    "            'sql': \"ALTER TABLE obstetricos ADD CONSTRAINT pk_obstetricos PRIMARY KEY (N_AIH);\",\n",
    "            'desc': \"PK obstetricos\",\n",
    "            'tipo': \"PK\"\n",
    "        },\n",
    "        {\n",
    "            'sql': \"ALTER TABLE procedimentos ADD CONSTRAINT pk_procedimentos PRIMARY KEY (PROC_REA);\",\n",
    "            'desc': \"PK procedimentos\",\n",
    "            'tipo': \"PK\"\n",
    "        },\n",
    "        {\n",
    "            'sql': \"ALTER TABLE municipios ADD CONSTRAINT pk_municipios PRIMARY KEY (codigo_municipio_6d);\",\n",
    "            'desc': \"PK municipios\",\n",
    "            'tipo': \"PK\"\n",
    "        },\n",
    "        \n",
    "        # Chaves estrangeiras - relacionamentos principais\n",
    "        {\n",
    "            'sql': \"ALTER TABLE financeiro ADD CONSTRAINT fk_financeiro_internacoes FOREIGN KEY (N_AIH) REFERENCES internacoes(N_AIH) ON DELETE CASCADE;\",\n",
    "            'desc': \"FK financeiro → internacoes\",\n",
    "            'tipo': \"FK\"\n",
    "        },\n",
    "        {\n",
    "            'sql': \"ALTER TABLE uti_info ADD CONSTRAINT fk_uti_internacoes FOREIGN KEY (N_AIH) REFERENCES internacoes(N_AIH) ON DELETE CASCADE;\",\n",
    "            'desc': \"FK uti_info → internacoes\",\n",
    "            'tipo': \"FK\"\n",
    "        },\n",
    "        {\n",
    "            'sql': \"ALTER TABLE obstetricos ADD CONSTRAINT fk_obs_internacoes FOREIGN KEY (N_AIH) REFERENCES internacoes(N_AIH) ON DELETE CASCADE;\",\n",
    "            'desc': \"FK obstetricos → internacoes\",\n",
    "            'tipo': \"FK\"\n",
    "        },\n",
    "        \n",
    "        # Chaves estrangeiras - tabelas de apoio\n",
    "        {\n",
    "            'sql': \"ALTER TABLE internacoes ADD CONSTRAINT fk_internacoes_procedimentos FOREIGN KEY (PROC_REA) REFERENCES procedimentos(PROC_REA);\",\n",
    "            'desc': \"FK internacoes → procedimentos\",\n",
    "            'tipo': \"FK\"\n",
    "        },\n",
    "        {\n",
    "            'sql': \"ALTER TABLE internacoes ADD CONSTRAINT fk_internacoes_municipios FOREIGN KEY (MUNIC_RES) REFERENCES municipios(codigo_municipio_6d);\",\n",
    "            'desc': \"FK internacoes → municipios\",\n",
    "            'tipo': \"FK\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    sucessos = {'PK': 0, 'FK': 0}\n",
    "    erros = {'PK': 0, 'FK': 0}\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        for comando in comandos_sql:\n",
    "            try:\n",
    "                conn.execute(text(comando['sql']))\n",
    "                log_operacao(f\"{comando['desc']}\")\n",
    "                sucessos[comando['tipo']] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                erro_msg = str(e)[:100] + \"...\" if len(str(e)) > 100 else str(e)\n",
    "                log_operacao(f\"{comando['desc']}: {erro_msg}\", \"WARNING\")\n",
    "                erros[comando['tipo']] += 1\n",
    "                continue\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    log_operacao(\"Resumo da criação de chaves:\")\n",
    "    log_operacao(f\"   Chaves primárias: {sucessos['PK']}/6 criadas\")\n",
    "    log_operacao(f\"   Chaves estrangeiras: {sucessos['FK']}/5 criadas\")\n",
    "    \n",
    "    if erros['PK'] > 0 or erros['FK'] > 0:\n",
    "        log_operacao(f\"Erros: {erros['PK']} PK, {erros['FK']} FK\", \"WARNING\")\n",
    "    else:\n",
    "        log_operacao(\"Todas as chaves criadas com sucesso!\")\n",
    "    \n",
    "    return sucessos, erros\n",
    "\n",
    "def executar_analise_procedimentos(engine):\n",
    "    \"\"\"Análise completa da tabela de procedimentos - 'correr da tabela'\"\"\"\n",
    "    log_operacao(\"Executando análise completa de procedimentos...\")\n",
    "    \n",
    "    try:\n",
    "        log_operacao(\"Verificando integridade referencial...\")\n",
    "        \n",
    "        orfaos = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                i.PROC_REA,\n",
    "                COUNT(*) as qtd_internacoes\n",
    "            FROM internacoes i\n",
    "            LEFT JOIN procedimentos p ON i.PROC_REA = p.PROC_REA\n",
    "            WHERE p.PROC_REA IS NULL\n",
    "            GROUP BY i.PROC_REA\n",
    "            ORDER BY qtd_internacoes DESC\n",
    "            LIMIT 20\n",
    "        \"\"\", engine)\n",
    "        \n",
    "        if len(orfaos) > 0:\n",
    "            total_orfaos = orfaos['qtd_internacoes'].sum()\n",
    "            log_operacao(f\"{len(orfaos)} códigos sem nome cadastrado\", \"WARNING\")\n",
    "            log_operacao(f\"{total_orfaos:,} internações afetadas\")\n",
    "            \n",
    "            log_operacao(\"Top 5 procedimentos órfãos:\")\n",
    "            print(orfaos.head().to_string(index=False))\n",
    "        else:\n",
    "            log_operacao(\"Todos os procedimentos têm nome cadastrado!\")\n",
    "        \n",
    "        # Estatísticas para apresentação\n",
    "        stats_gerais = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(DISTINCT i.PROC_REA) as proc_usados_internacoes,\n",
    "                COUNT(DISTINCT p.PROC_REA) as proc_cadastrados,\n",
    "                (SELECT COUNT(*) FROM internacoes) as total_internacoes,\n",
    "                (SELECT COUNT(*) FROM procedimentos) as total_proc_cadastrados\n",
    "            FROM internacoes i\n",
    "            FULL OUTER JOIN procedimentos p ON i.PROC_REA = p.PROC_REA\n",
    "        \"\"\", engine)\n",
    "        \n",
    "        log_operacao(\"Estatísticas gerais:\")\n",
    "        log_operacao(f\"   Procedimentos usados: {stats_gerais['proc_usados_internacoes'].iloc[0]:,}\")\n",
    "        log_operacao(f\"   Procedimentos cadastrados: {stats_gerais['proc_cadastrados'].iloc[0]:,}\")\n",
    "        log_operacao(f\"   Total internações: {stats_gerais['total_internacoes'].iloc[0]:,}\")\n",
    "        \n",
    "        # Top procedimentos para análise de padrões\n",
    "        log_operacao(\"Analisando top procedimentos...\")\n",
    "        \n",
    "        top_proc = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                i.PROC_REA,\n",
    "                COALESCE(p.NOME_PROC, 'SEM_NOME') as nome_procedimento,\n",
    "                COUNT(*) as total_internacoes,\n",
    "                ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM internacoes), 2) as percentual,\n",
    "                ROUND(AVG(f.VAL_TOT), 2) as custo_medio,\n",
    "                ROUND(SUM(f.VAL_TOT), 2) as custo_total\n",
    "            FROM internacoes i\n",
    "            LEFT JOIN procedimentos p ON i.PROC_REA = p.PROC_REA\n",
    "            LEFT JOIN financeiro f ON i.N_AIH = f.N_AIH\n",
    "            GROUP BY i.PROC_REA, p.NOME_PROC\n",
    "            ORDER BY total_internacoes DESC\n",
    "            LIMIT 10\n",
    "        \"\"\", engine)\n",
    "        \n",
    "        log_operacao(\"Top 10 procedimentos mais realizados:\")\n",
    "        print(top_proc.to_string(index=False, max_colwidth=30))\n",
    "        \n",
    "        try:\n",
    "            tendencia_anual = pd.read_sql(\"\"\"\n",
    "                SELECT \n",
    "                    EXTRACT(YEAR FROM DT_INTER) as ano,\n",
    "                    COUNT(*) as total_internacoes,\n",
    "                    COUNT(DISTINCT PROC_REA) as procedimentos_distintos\n",
    "                FROM internacoes\n",
    "                WHERE DT_INTER IS NOT NULL\n",
    "                GROUP BY EXTRACT(YEAR FROM DT_INTER)\n",
    "                ORDER BY ano DESC\n",
    "                LIMIT 5\n",
    "            \"\"\", engine)\n",
    "            \n",
    "            if len(tendencia_anual) > 0:\n",
    "                log_operacao(\"Tendência por ano:\")\n",
    "                print(tendencia_anual.to_string(index=False))\n",
    "            \n",
    "        except Exception:\n",
    "            log_operacao(\"Dados temporais indisponíveis\", \"WARNING\")\n",
    "        \n",
    "        log_operacao(\"Gerando relatório detalhado...\")\n",
    "        \n",
    "        relatorio_detalhado = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                i.PROC_REA,\n",
    "                COALESCE(p.NOME_PROC, 'SEM_NOME') as nome_procedimento,\n",
    "                COUNT(*) as total_internacoes,\n",
    "                COUNT(DISTINCT i.MUNIC_RES) as municipios_atendidos,\n",
    "                ROUND(AVG(i.IDADE), 1) as idade_media,\n",
    "                ROUND(AVG(i.DIAS_PERM), 1) as dias_perm_medio,\n",
    "                COUNT(CASE WHEN i.MORTE = 1 THEN 1 END) as obitos,\n",
    "                ROUND(COUNT(CASE WHEN i.MORTE = 1 THEN 1 END) * 100.0 / COUNT(*), 2) as taxa_mortalidade,\n",
    "                ROUND(AVG(f.VAL_TOT), 2) as custo_medio,\n",
    "                ROUND(SUM(f.VAL_TOT), 2) as custo_total\n",
    "            FROM internacoes i\n",
    "            LEFT JOIN procedimentos p ON i.PROC_REA = p.PROC_REA\n",
    "            LEFT JOIN financeiro f ON i.N_AIH = f.N_AIH\n",
    "            GROUP BY i.PROC_REA, p.NOME_PROC\n",
    "            HAVING COUNT(*) >= 10\n",
    "            ORDER BY total_internacoes DESC\n",
    "            LIMIT 100\n",
    "        \"\"\", engine)\n",
    "        \n",
    "        relatorio_path = \"relatorio_procedimentos_completo.csv\"\n",
    "        relatorio_detalhado.to_csv(relatorio_path, index=False)\n",
    "        log_operacao(f\"Relatório salvo: {relatorio_path}\")\n",
    "        \n",
    "        if len(orfaos) > 0:\n",
    "            log_operacao(\"Corrigindo procedimentos órfãos...\")\n",
    "            \n",
    "            with engine.connect() as conn:\n",
    "                for _, row in orfaos.iterrows():\n",
    "                    proc_rea = row['PROC_REA']\n",
    "                    nome_generico = f\"PROCEDIMENTO_{proc_rea}\"\n",
    "                    \n",
    "                    conn.execute(text(\"\"\"\n",
    "                        INSERT INTO procedimentos (PROC_REA, NOME_PROC) \n",
    "                        VALUES (:proc_rea, :nome_proc)\n",
    "                        ON CONFLICT (PROC_REA) DO NOTHING\n",
    "                    \"\"\"), {'proc_rea': proc_rea, 'nome_proc': nome_generico})\n",
    "                \n",
    "                conn.commit()\n",
    "                log_operacao(f\"{len(orfaos)} procedimentos órfãos corrigidos!\")\n",
    "        \n",
    "        log_operacao(\"Análise de procedimentos concluída!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_operacao(f\"Erro na análise de procedimentos: {e}\", \"ERROR\")\n",
    "        return False\n",
    "\n",
    "def verificacoes_finais(engine):\n",
    "    \"\"\"Verifica integridade final do banco\"\"\"\n",
    "    log_operacao(\"Executando verificações finais...\")\n",
    "    \n",
    "    tabelas_esperadas = ['internacoes', 'financeiro', 'uti_info', 'obstetricos', \n",
    "                        'procedimentos', 'municipios', 'cid10']\n",
    "    \n",
    "    log_operacao(\"Contagem de registros por tabela:\")\n",
    "    tabelas_existentes = []\n",
    "    \n",
    "    for tabela in tabelas_esperadas:\n",
    "        try:\n",
    "            count = pd.read_sql(f\"SELECT COUNT(*) as total FROM {tabela}\", engine)\n",
    "            total_registros = count['total'].iloc[0]\n",
    "            log_operacao(f\"   {tabela:15}: {total_registros:>10,} registros\")\n",
    "            tabelas_existentes.append(tabela)\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_operacao(f\"   {tabela:15}: Erro ou não existe\", \"WARNING\")\n",
    "    \n",
    "    log_operacao(\"Verificando relacionamentos:\")\n",
    "    \n",
    "    try:\n",
    "        join_test = pd.read_sql(\"\"\"\n",
    "            SELECT i.N_AIH, i.PROC_REA, f.VAL_TOT \n",
    "            FROM internacoes i \n",
    "            JOIN financeiro f ON i.N_AIH = f.N_AIH \n",
    "            LIMIT 5\n",
    "        \"\"\", engine)\n",
    "        \n",
    "        if len(join_test) > 0:\n",
    "            log_operacao(\"Join internacoes ↔ financeiro funcionando\")\n",
    "        \n",
    "        orfaos_financeiro = pd.read_sql(\"\"\"\n",
    "            SELECT COUNT(*) as orfaos \n",
    "            FROM financeiro f \n",
    "            LEFT JOIN internacoes i ON f.N_AIH = i.N_AIH \n",
    "            WHERE i.N_AIH IS NULL\n",
    "        \"\"\", engine)\n",
    "        \n",
    "        orfaos_count = orfaos_financeiro['orfaos'].iloc[0]\n",
    "        if orfaos_count == 0:\n",
    "            log_operacao(\"Integridade referencial OK (financeiro)\")\n",
    "        else:\n",
    "            log_operacao(f\"{orfaos_count} registros órfãos em financeiro\", \"WARNING\")\n",
    "        \n",
    "        if 'procedimentos' in tabelas_existentes:\n",
    "            proc_join = pd.read_sql(\"\"\"\n",
    "                SELECT i.PROC_REA, p.NOME_PROC, COUNT(*) as casos\n",
    "                FROM internacoes i\n",
    "                LEFT JOIN procedimentos p ON i.PROC_REA = p.PROC_REA\n",
    "                GROUP BY i.PROC_REA, p.NOME_PROC\n",
    "                ORDER BY casos DESC\n",
    "                LIMIT 5\n",
    "            \"\"\", engine)\n",
    "            \n",
    "            if len(proc_join) > 0:\n",
    "                log_operacao(\"Join internacoes ↔ procedimentos funcionando\")\n",
    "                sem_nome = proc_join[proc_join['NOME_PROC'].isnull()]\n",
    "                if len(sem_nome) > 0:\n",
    "                    log_operacao(f\"{len(sem_nome)} procedimentos sem nome no top 5\", \"WARNING\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_operacao(f\"Erro na verificação de relacionamentos: {e}\", \"ERROR\")\n",
    "    \n",
    "    log_operacao(\"Amostra da tabela internacoes:\")\n",
    "    try:\n",
    "        sample = pd.read_sql(\"\"\"\n",
    "            SELECT N_AIH, PROC_REA, MUNIC_RES, IDADE, SEXO, MORTE, DIAS_PERM\n",
    "            FROM internacoes \n",
    "            ORDER BY N_AIH \n",
    "            LIMIT 5\n",
    "        \"\"\", engine)\n",
    "        print(sample.to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_operacao(f\"Erro ao buscar amostra: {e}\", \"ERROR\")\n",
    "    \n",
    "    # Estatísticas finais para relatórios\n",
    "    log_operacao(\"Estatísticas finais do banco:\")\n",
    "    try:\n",
    "        stats_finais = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                (SELECT COUNT(*) FROM internacoes) as total_internacoes,\n",
    "                (SELECT COUNT(DISTINCT PROC_REA) FROM internacoes) as procedimentos_distintos,\n",
    "                (SELECT COUNT(DISTINCT MUNIC_RES) FROM internacoes) as municipios_distintos,\n",
    "                (SELECT ROUND(AVG(IDADE), 1) FROM internacoes WHERE IDADE > 0) as idade_media,\n",
    "                (SELECT COUNT(*) FROM internacoes WHERE MORTE = 1) as total_obitos\n",
    "        \"\"\", engine)\n",
    "        \n",
    "        stats = stats_finais.iloc[0]\n",
    "        log_operacao(f\"   Total internações: {stats['total_internacoes']:,}\")\n",
    "        log_operacao(f\"   Procedimentos distintos: {stats['procedimentos_distintos']:,}\")\n",
    "        log_operacao(f\"   Municípios distintos: {stats['municipios_distintos']:,}\")\n",
    "        log_operacao(f\"   Idade média: {stats['idade_media']} anos\")\n",
    "        log_operacao(f\"   Total óbitos: {stats['total_obitos']:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_operacao(f\"Erro nas estatísticas finais: {e}\", \"ERROR\")\n",
    "    \n",
    "    log_operacao(\"Verificações finais concluídas!\")\n",
    "\n",
    "def configurar_atualizacao_automatica(engine):\n",
    "    \"\"\"Configura scripts de atualização automática para produção\"\"\"\n",
    "    log_operacao(\"Configurando atualização automática...\")\n",
    "    \n",
    "    try:\n",
    "        info_atual = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                MAX(DT_INTER) as ultima_internacao,\n",
    "                MIN(DT_INTER) as primeira_internacao,\n",
    "                COUNT(*) as total_registros,\n",
    "                COUNT(DISTINCT MUNIC_RES) as municipios_distintos,\n",
    "                COUNT(DISTINCT PROC_REA) as procedimentos_distintos\n",
    "            FROM internacoes\n",
    "            WHERE DT_INTER IS NOT NULL\n",
    "        \"\"\", engine)\n",
    "        \n",
    "        log_operacao(\"Informações atuais do banco:\")\n",
    "        stats = info_atual.iloc[0]\n",
    "        log_operacao(f\"   Primeira internação: {stats['primeira_internacao']}\")\n",
    "        log_operacao(f\"   Última internação: {stats['ultima_internacao']}\")\n",
    "        log_operacao(f\"   Total registros: {stats['total_registros']:,}\")\n",
    "        log_operacao(f\"   Municípios distintos: {stats['municipios_distintos']:,}\")\n",
    "        log_operacao(f\"   Procedimentos distintos: {stats['procedimentos_distintos']:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_operacao(f\"Erro ao verificar informações atuais: {e}\", \"ERROR\")\n",
    "    \n",
    "    # Script de atualização para produção\n",
    "    script_atualizacao = f\"\"\"#!/bin/bash\n",
    "# Script de atualização automática SIH-RS\n",
    "# Gerado em {datetime.now()}\n",
    "\n",
    "LOG_DIR=\"../../logs\"\n",
    "BACKUP_DIR=\"../../backups\"\n",
    "PROJETO_DIR=\"$(cd \"$(dirname \"${{BASH_SOURCE[0]}}\")\" && pwd)\"\n",
    "TIMESTAMP=$(date +%Y%m%d_%H%M%S)\n",
    "\n",
    "mkdir -p \"$LOG_DIR\"\n",
    "mkdir -p \"$BACKUP_DIR\"\n",
    "\n",
    "LOG_FILE=\"$LOG_DIR/atualizacao_$TIMESTAMP.log\"\n",
    "\n",
    "echo \"=== INICIANDO ATUALIZAÇÃO SIH-RS $TIMESTAMP ===\" | tee -a \"$LOG_FILE\"\n",
    "\n",
    "cd \"$PROJETO_DIR\"\n",
    "\n",
    "# Executar pré-tratamento\n",
    "echo \"Executando pré-tratamento...\" | tee -a \"$LOG_FILE\"\n",
    "python3 pre_tratamento.py >> \"$LOG_FILE\" 2>&1\n",
    "\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Pré-tratamento concluído\" | tee -a \"$LOG_FILE\"\n",
    "else\n",
    "    echo \"Erro no pré-tratamento\" | tee -a \"$LOG_FILE\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Executar atualização do banco\n",
    "echo \"Executando atualização do banco...\" | tee -a \"$LOG_FILE\"\n",
    "python3 bd.py >> \"$LOG_FILE\" 2>&1\n",
    "\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Banco atualizado com sucesso\" | tee -a \"$LOG_FILE\"\n",
    "    \n",
    "    # Backup do banco\n",
    "    echo \"Criando backup...\" | tee -a \"$LOG_FILE\"\n",
    "    pg_dump -U {DB_CONFIG['usuario']} -h {DB_CONFIG['host']} \\\\\n",
    "            -p {DB_CONFIG['porta']} {DB_CONFIG['banco']} > \"$BACKUP_DIR/sih_rs_$TIMESTAMP.sql\"\n",
    "    \n",
    "    if [ $? -eq 0 ]; then\n",
    "        echo \"Backup criado: $BACKUP_DIR/sih_rs_$TIMESTAMP.sql\" | tee -a \"$LOG_FILE\"\n",
    "        \n",
    "        # Manter apenas os últimos 10 backups\n",
    "        find \"$BACKUP_DIR\" -name \"sih_rs_*.sql\" -type f | sort -r | tail -n +11 | xargs rm -f\n",
    "        echo \"Backups antigos removidos\" | tee -a \"$LOG_FILE\"\n",
    "    else\n",
    "        echo \"Erro ao criar backup\" | tee -a \"$LOG_FILE\"\n",
    "    fi\n",
    "    \n",
    "else\n",
    "    echo \"Erro na atualização do banco\" | tee -a \"$LOG_FILE\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"=== ATUALIZAÇÃO CONCLUÍDA $TIMESTAMP ===\" | tee -a \"$LOG_FILE\"\n",
    "\"\"\"\n",
    "    \n",
    "    script_path = \"atualizar_sih_automatico.sh\"\n",
    "    with open(script_path, 'w') as f:\n",
    "        f.write(script_atualizacao)\n",
    "    \n",
    "    os.chmod(script_path, 0o755)\n",
    "    log_operacao(f\"Script criado: {script_path}\")\n",
    "    \n",
    "    # Script de backup independente\n",
    "    script_backup = f\"\"\"#!/bin/bash\n",
    "# Script de backup independente SIH-RS\n",
    "BACKUP_DIR=\"../../backups\"\n",
    "TIMESTAMP=$(date +%Y%m%d_%H%M%S)\n",
    "\n",
    "mkdir -p \"$BACKUP_DIR\"\n",
    "\n",
    "echo \"Iniciando backup SIH-RS...\"\n",
    "pg_dump -U {DB_CONFIG['usuario']} -h {DB_CONFIG['host']} \\\\\n",
    "        -p {DB_CONFIG['porta']} {DB_CONFIG['banco']} > \"$BACKUP_DIR/sih_rs_$TIMESTAMP.sql\"\n",
    "\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Backup criado: $BACKUP_DIR/sih_rs_$TIMESTAMP.sql\"\n",
    "    \n",
    "    # Comprimir backup\n",
    "    gzip \"$BACKUP_DIR/sih_rs_$TIMESTAMP.sql\"\n",
    "    echo \"Backup comprimido\"\n",
    "    \n",
    "    # Manter apenas últimos 7 backups\n",
    "    find \"$BACKUP_DIR\" -name \"sih_rs_*.sql.gz\" -type f -mtime +7 -delete\n",
    "    echo \"Backups antigos removidos\"\n",
    "else\n",
    "    echo \"Erro no backup\"\n",
    "    exit 1\n",
    "fi\n",
    "\"\"\"\n",
    "    \n",
    "    backup_script_path = \"backup_sih.sh\"\n",
    "    with open(backup_script_path, 'w') as f:\n",
    "        f.write(script_backup)\n",
    "    \n",
    "    os.chmod(backup_script_path, 0o755)\n",
    "    log_operacao(f\"Script de backup criado: {backup_script_path}\")\n",
    "    \n",
    "    log_operacao(\"Instruções para agendamento:\")\n",
    "    log_operacao(\"   1. Editar crontab: crontab -e\")\n",
    "    log_operacao(\"   2. Adicionar linha para atualização semanal:\")\n",
    "    log_operacao(f\"      0 2 * * 1 {os.path.abspath(script_path)}\")\n",
    "    log_operacao(\"   3. Adicionar linha para backup diário:\")\n",
    "    log_operacao(f\"      0 1 * * * {os.path.abspath(backup_script_path)}\")\n",
    "    log_operacao(\"   4. Verificar agendamento: crontab -l\")\n",
    "    \n",
    "    # Arquivo de configuração para equipe\n",
    "    config_atualizacao = f\"\"\"# Configuração de Atualização Automática SIH-RS\n",
    "# Gerado em {datetime.now()}\n",
    "\n",
    "# Horários recomendados:\n",
    "# - Backup diário: 01:00 (0 1 * * *)\n",
    "# - Atualização semanal: 02:00 Segunda-feira (0 2 * * 1)\n",
    "\n",
    "# Configurações do banco\n",
    "BANCO_HOST={DB_CONFIG['host']}\n",
    "BANCO_PORTA={DB_CONFIG['porta']}\n",
    "BANCO_NOME={DB_CONFIG['banco']}\n",
    "BANCO_USUARIO={DB_CONFIG['usuario']}\n",
    "\n",
    "# Diretórios\n",
    "LOG_DIR=../../logs\n",
    "BACKUP_DIR=../../backups\n",
    "DADOS_DIR=../../banco\n",
    "\n",
    "# Retenção de backups (dias)\n",
    "BACKUP_RETENCAO=7\n",
    "\"\"\"\n",
    "    \n",
    "    with open(\"config_atualizacao.conf\", 'w') as f:\n",
    "        f.write(config_atualizacao)\n",
    "    \n",
    "    log_operacao(\"Arquivo de configuração criado: config_atualizacao.conf\")\n",
    "    log_operacao(\"Atualização automática configurada!\")\n",
    "\n",
    "def executar_pipeline_completo():\n",
    "    \"\"\"Pipeline principal que implementa todas as modificações solicitadas\"\"\"\n",
    "    inicio = datetime.now()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"           PIPELINE BANCO SIH-RS NORMALIZADO\")\n",
    "    print(\"                   VERSÃO FINAL\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    log_operacao(\"INICIANDO PIPELINE COMPLETO\")\n",
    "    log_operacao(f\"Data/hora: {inicio}\")\n",
    "    verificar_memoria()\n",
    "    \n",
    "    try:\n",
    "        # 1. Conectar e validar\n",
    "        log_operacao(\"ETAPA 1: CONEXÃO E VALIDAÇÕES\")\n",
    "        engine = conectar_validar()\n",
    "        \n",
    "        # 2. Carregar dados principais\n",
    "        log_operacao(\"ETAPA 2: CARREGAMENTO DE DADOS\")\n",
    "        df = carregar_dados_principais()\n",
    "        \n",
    "        # 3. Criar tabelas normalizadas\n",
    "        log_operacao(\"ETAPA 3: CRIAÇÃO DE TABELAS NORMALIZADAS\")\n",
    "        stats_principais = criar_tabelas_normalizadas(engine, df)\n",
    "        \n",
    "        del df\n",
    "        limpar_memoria()\n",
    "        \n",
    "        # 4. Carregar tabelas de apoio\n",
    "        log_operacao(\"ETAPA 4: TABELAS DE APOIO\")\n",
    "        stats_apoio = carregar_tabelas_apoio(engine)\n",
    "        \n",
    "        # 5. Criar chaves e relacionamentos\n",
    "        log_operacao(\"ETAPA 5: CHAVES E RELACIONAMENTOS\")\n",
    "        sucessos_chaves, erros_chaves = criar_chaves_relacionamentos(engine)\n",
    "        \n",
    "        # 6. Executar análise de procedimentos\n",
    "        log_operacao(\"ETAPA 6: ANÁLISE DE PROCEDIMENTOS\")\n",
    "        analise_sucesso = executar_analise_procedimentos(engine)\n",
    "        \n",
    "        # 7. Verificações finais\n",
    "        log_operacao(\"ETAPA 7: VERIFICAÇÕES FINAIS\")\n",
    "        verificacoes_finais(engine)\n",
    "        \n",
    "        # 8. Configurar atualização automática\n",
    "        log_operacao(\"ETAPA 8: ATUALIZAÇÃO AUTOMÁTICA\")\n",
    "        configurar_atualizacao_automatica(engine)\n",
    "        \n",
    "        # Relatório final\n",
    "        duracao = datetime.now() - inicio\n",
    "        \n",
    "        log_operacao(\"\\n\" + \"=\"*70)\n",
    "        log_operacao(\"PIPELINE CONCLUÍDO COM SUCESSO!\")\n",
    "        log_operacao(\"=\"*70)\n",
    "        \n",
    "        log_operacao(f\"Duração total: {duracao}\")\n",
    "        log_operacao(f\"Tabelas principais criadas: {len(stats_principais)}\")\n",
    "        log_operacao(f\"Tabelas de apoio carregadas: {len(stats_apoio)}\")\n",
    "        log_operacao(f\"Chaves primárias: {sucessos_chaves.get('PK', 0)}/6\")\n",
    "        log_operacao(f\"Chaves estrangeiras: {sucessos_chaves.get('FK', 0)}/5\")\n",
    "        log_operacao(f\"Análise de procedimentos: {'Concluída' if analise_sucesso else 'Com erros'}\")\n",
    "        \n",
    "        log_operacao(\"\\nArquivos gerados:\")\n",
    "        log_operacao(\"   • atualizar_sih_automatico.sh\")\n",
    "        log_operacao(\"   • backup_sih.sh\")\n",
    "        log_operacao(\"   • config_atualizacao.conf\")\n",
    "        log_operacao(\"   • relatorio_procedimentos_completo.csv\")\n",
    "        \n",
    "        log_operacao(\"\\nConectar ao banco:\")\n",
    "        log_operacao(f\"   psql -U {DB_CONFIG['usuario']} -h {DB_CONFIG['host']} -d {DB_CONFIG['banco']}\")\n",
    "        \n",
    "        log_operacao(\"\\nPróximos passos:\")\n",
    "        log_operacao(\"   1. Configurar agendamento: crontab -e\")\n",
    "        log_operacao(\"   2. Testar scripts de backup\")\n",
    "        log_operacao(\"   3. Validar integridade dos dados\")\n",
    "        \n",
    "        log_operacao(\"=\"*70)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        duracao = datetime.now() - inicio\n",
    "        log_operacao(f\"\\nERRO CRÍTICO NO PIPELINE: {e}\", \"ERROR\")\n",
    "        log_operacao(f\"Duração até erro: {duracao}\")\n",
    "        log_operacao(\"=\"*70)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        sucesso = executar_pipeline_completo()\n",
    "        if sucesso:\n",
    "            print(\"\\nEXECUÇÃO CONCLUÍDA COM SUCESSO!\")\n",
    "            print(\"Verifique os logs e arquivos gerados\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERRO NA EXECUÇÃO: {e}\")\n",
    "        print(\"Verifique os logs para mais detalhes\")\n",
    "        verificar_memoria()\n",
    "        \n",
    "    finally:\n",
    "        limpar_memoria()\n",
    "        print(f\"\\nExecução finalizada em {datetime.now()}\")\n",
    "\n",
    "# Comandos úteis para teste no Jupyter:\n",
    "# engine = conectar_validar()\n",
    "# df_sample = pd.read_parquet(PARQUET_PATH, engine=\"pyarrow\").head(10000)\n",
    "# criar_tabelas_normalizadas(engine, df_sample)\n",
    "# pd.read_sql(\"SELECT COUNT(*) FROM internacoes\", engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

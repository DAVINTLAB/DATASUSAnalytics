{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 670 arquivos parquet\n",
      "Iniciando processamento...\n",
      "Processando arquivo 1/670: RDRS1807.parquet\n",
      "Processando arquivo 51/670: RDRS1008.parquet\n",
      "Processando arquivo 101/670: RDRS1005.parquet\n",
      "Processando arquivo 151/670: RDRS1105.parquet\n",
      "Processando arquivo 201/670: 1f0486fed8374f8a907e594c73dc17b1-0.parquet\n",
      "Processando arquivo 251/670: 1e5d29aac9df43ea80fb23fca5b7c6ca-0.parquet\n",
      "Processando arquivo 301/670: 0130edbbc51f40fc8db71e5b8c155d7f-0.parquet\n",
      "Processando arquivo 351/670: 745dd286a8ff461896c8180872916cbf-0.parquet\n",
      "Processando arquivo 401/670: ee4e4dd3fa254661931c712f86facc7e-0.parquet\n",
      "Processando arquivo 451/670: 20050fbf622a40769081cc40095922f2-0.parquet\n",
      "Processando arquivo 501/670: 8c7a3bd79bd14fdebb859e7dfa83fd9e-0.parquet\n"
     ]
    }
   ],
   "source": [
    "# UNIFICAR OS PARQUETS - VERSÃO LIMPA\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Caminhos\n",
    "pasta_parquets = \"../../banco/parquet\"\n",
    "saida_parquet = \"../../banco/parquet_unificado/sih_rs.parquet\"\n",
    "\n",
    "# Colunas desejadas (corrigida DT_INTER)\n",
    "colunas_desejadas = [\n",
    "     'ESPEC', 'N_AIH', 'IDENT', 'CEP', 'MUNIC_RES', 'NASC', 'SEXO', 'DT_INTER', 'DT_SAIDA',\n",
    "    'UTI_MES_TO', 'MARCA_UTI', 'UTI_INT_TO', 'DIAR_ACOM', 'QT_DIARIAS', 'PROC_SOLIC', 'PROC_REA', 'VAL_SH', 'VAL_SP', 'VAL_TOT',\n",
    "    'VAL_UTI',  'NATUREZA', 'CNES',  'NAT_JUR', 'GESTAO', 'IND_VDRL', 'IDADE', 'DIAG_PRINC', \n",
    "    'DIAG_SECUN', 'COBRANCA', 'MORTE',  'MUNIC_MOV', 'DIAS_PERM', 'NACIONAL', \n",
    "    'NUM_FILHOS', 'INSTRU', 'CID_NOTIF', 'CONTRACEP1', 'CONTRACEP2', 'GESTRICO', 'INSC_PN', 'CBOR',\n",
    "    'CNAER', 'VINCPREV', 'INFEHOSP', 'CID_ASSO', 'CID_MORTE', 'COMPLEX', 'RACA_COR', 'ETNIA',\n",
    "    'DIAGSEC1', 'DIAGSEC2', 'DIAGSEC3', 'DIAGSEC4', 'DIAGSEC5', 'DIAGSEC6', 'DIAGSEC7', 'DIAGSEC8', 'DIAGSEC9', \n",
    "    'TPDISEC1', 'TPDISEC2', 'TPDISEC3', 'TPDISEC4', 'TPDISEC5', 'TPDISEC6', 'TPDISEC7', 'TPDISEC8', 'TPDISEC9', 'COD_IDADE'\n",
    "]\n",
    "\n",
    "# Buscar arquivos parquet recursivamente\n",
    "arquivos = glob(os.path.join(pasta_parquets, \"**/*.parquet\"), recursive=True)\n",
    "\n",
    "if len(arquivos) == 0:\n",
    "    print(\"ERRO: Nenhum arquivo .parquet encontrado!\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Encontrados {len(arquivos)} arquivos parquet\")\n",
    "print(\"Iniciando processamento...\")\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "dfs = []\n",
    "arquivos_processados = 0\n",
    "arquivos_com_erro = 0\n",
    "\n",
    "# Itera sobre os arquivos .parquet\n",
    "for i, arquivo in enumerate(arquivos):\n",
    "    # Mostrar progresso a cada 50 arquivos ou nos últimos 10\n",
    "    if i % 50 == 0 or i >= len(arquivos) - 10:\n",
    "        print(f\"Processando arquivo {i+1}/{len(arquivos)}: {os.path.basename(arquivo)}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_parquet(arquivo)\n",
    "        \n",
    "        # Adiciona colunas ausentes com NaN\n",
    "        for coluna in colunas_desejadas:\n",
    "            if coluna not in df.columns:\n",
    "                df[coluna] = pd.NA\n",
    "\n",
    "        # Mantém apenas as colunas desejadas \n",
    "        df = df[colunas_desejadas]\n",
    "        \n",
    "        dfs.append(df)\n",
    "        arquivos_processados += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERRO no arquivo {os.path.basename(arquivo)}: {e}\")\n",
    "        arquivos_com_erro += 1\n",
    "        continue\n",
    "\n",
    "print(f\"\\nResumo do processamento:\")\n",
    "print(f\"  Arquivos processados: {arquivos_processados}/{len(arquivos)}\")\n",
    "if arquivos_com_erro > 0:\n",
    "    print(f\"  Arquivos com erro: {arquivos_com_erro}\")\n",
    "\n",
    "if len(dfs) == 0:\n",
    "    print(\"ERRO: Nenhum arquivo foi processado com sucesso!\")\n",
    "else:\n",
    "    print(f\"\\nConcatenando {len(dfs)} DataFrames...\")\n",
    "    \n",
    "    # Concatena em lotes para evitar sobrecarga de memória\n",
    "    lote_size = 50\n",
    "    df_lotes = []\n",
    "    \n",
    "    for i in range(0, len(dfs), lote_size):\n",
    "        lote = dfs[i:i+lote_size]\n",
    "        print(f\"  Concatenando lote {i//lote_size + 1}/{(len(dfs)-1)//lote_size + 1}\")\n",
    "        df_lote = pd.concat(lote, ignore_index=True)\n",
    "        df_lotes.append(df_lote)\n",
    "        \n",
    "        # Limpar memória\n",
    "        del lote\n",
    "    \n",
    "    print(\"  Concatenação final...\")\n",
    "    df_final = pd.concat(df_lotes, ignore_index=True)\n",
    "    \n",
    "    # Limpar memória\n",
    "    del dfs, df_lotes\n",
    "    \n",
    "    # Cria a pasta de saída, se necessário\n",
    "    os.makedirs(os.path.dirname(saida_parquet), exist_ok=True)\n",
    "\n",
    "    # Salva o arquivo unificado\n",
    "    print(\"Salvando arquivo unificado...\")\n",
    "    df_final.to_parquet(saida_parquet, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "\n",
    "    print(f\"\\nSUCESSO!\")\n",
    "    print(f\"Arquivo criado: sih_rs.parquet\")\n",
    "    print(f\"Total de registros: {len(df_final):,}\")\n",
    "    print(f\"Tamanho: {os.path.getsize(saida_parquet) / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082e7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cabf036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b381f-fae5-47ab-a7b2-381a031ca4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

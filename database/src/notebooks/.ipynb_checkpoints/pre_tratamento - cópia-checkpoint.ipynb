{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e4af8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do arquivo: 792.4 MB\n",
      "Arquivo grande detectado (>500MB) - usando processamento em chunks\n",
      "Processando em chunks de 500,000 registros...\n",
      "Processando chunk 1 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 2 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 3 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 4 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 5 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 6 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 7 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 8 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 9 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 10 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 11 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 12 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 13 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 14 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 15 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 16 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 17 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 18 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 19 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 20 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 21 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 22 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 23 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 24 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 25 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 26 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 27 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 28 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 29 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 30 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 31 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 32 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 33 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 34 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 35 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 36 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 37 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 38 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 39 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 40 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 41 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 42 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 43 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 44 (500,000 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Processando chunk 45 (183,490 registros)...\n",
      "Tratando 14 variáveis CID...\n",
      "Tratando coluna SEXO...\n",
      "Tratando coluna VAL_SH...\n",
      "Tratando coluna VAL_SP...\n",
      "Tratando coluna VAL_TOT...\n",
      "Tratando coluna VAL_UTI...\n",
      "Tratando coluna UTI_MES_TO...\n",
      "Tratando coluna UTI_INT_TO...\n",
      "Tratando coluna DIAR_ACOM...\n",
      "Tratando coluna QT_DIARIAS...\n",
      "Tratando coluna DIAS_PERM...\n",
      "Arquivo tratado salvo: ../../banco/parquet_unificado/sih_rs_tratado.parquet\n",
      "Total de registros processados: 22,183,490\n",
      "\n",
      "Tratamento concluído com sucesso!\n",
      "Arquivo final: 728.6 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Funções de tratamento otimizadas\n",
    "def tratar_num_filhos(df):\n",
    "    \"\"\"Trata NUM_FILHOS com arredondamento da média para melhor precisão\"\"\"\n",
    "    if 'NUM_FILHOS' in df.columns:\n",
    "        df['NUM_FILHOS'] = pd.to_numeric(df['NUM_FILHOS'], errors='coerce').fillna(0)\n",
    "        # Calcula média dos valores válidos e arredonda (melhor da V1)\n",
    "        media_filhos = df.loc[df['NUM_FILHOS'] <= 20, 'NUM_FILHOS'].mean()\n",
    "        media_filhos = int(round(media_filhos)) if not pd.isna(media_filhos) else 0\n",
    "        df.loc[df['NUM_FILHOS'] > 20, 'NUM_FILHOS'] = media_filhos\n",
    "        df['NUM_FILHOS'] = df['NUM_FILHOS'].astype(int)\n",
    "    return df\n",
    "\n",
    "def tratar_instrucao(df):\n",
    "    \"\"\"Trata INSTRU com verificação de coluna existente\"\"\"\n",
    "    if 'INSTRU' not in df.columns:\n",
    "        return df\n",
    "    df['INSTRU'] = df['INSTRU'].astype(str).str.zfill(2)\n",
    "    df['INSTRU'] = df['INSTRU'].replace(['00', 'nan'], '0')\n",
    "    return df\n",
    "\n",
    "def padronizar_cids(df, colunas_cid):\n",
    "    \"\"\"Padroniza CIDs usando mask para evitar warnings\"\"\"\n",
    "    colunas_existentes = [col for col in colunas_cid if col in df.columns]\n",
    "    if not colunas_existentes:\n",
    "        return df\n",
    "    \n",
    "    print(f\"Tratando {len(colunas_existentes)} variáveis CID...\")\n",
    "    for col in colunas_existentes:\n",
    "        df[col] = df[col].astype(str).str.upper().str.strip()\n",
    "        # Usa mask para evitar FutureWarning (melhor da V1)\n",
    "        mask = df[col].isin(['0000', 'NAN', ''])\n",
    "        df.loc[mask, col] = np.nan\n",
    "    return df\n",
    "\n",
    "def tratar_cids(df):\n",
    "    \"\"\"Aplica padronização CID em todas as colunas relevantes\"\"\"\n",
    "    colunas_cid = [\n",
    "        'DIAG_PRINC', 'DIAG_SECUN', 'CID_NOTIF', 'CID_ASSO', 'CID_MORTE'\n",
    "    ] + [f'DIAGSEC{i}' for i in range(1, 10)]\n",
    "    return padronizar_cids(df, colunas_cid)\n",
    "\n",
    "def tratar_idade(df):\n",
    "    \"\"\"Trata idade mantendo como float para preservar precisão decimal\"\"\"\n",
    "    if 'IDADE' in df.columns:\n",
    "        # Mantém como float para melhor precisão \n",
    "        df['IDADE'] = pd.to_numeric(df['IDADE'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "    if 'COD_IDADE' in df.columns:\n",
    "        df['COD_IDADE'] = pd.to_numeric(df['COD_IDADE'], errors='coerce').fillna(0).astype(int)\n",
    "        \n",
    "        # Conversões de unidades de idade\n",
    "        df.loc[df['COD_IDADE'] == 1, 'IDADE'] = 0  \n",
    "        df.loc[df['COD_IDADE'] == 2, 'IDADE'] = (df.loc[df['COD_IDADE'] == 2, 'IDADE'] / 365).round(1) \n",
    "        df.loc[df['COD_IDADE'] == 3, 'IDADE'] = (df.loc[df['COD_IDADE'] == 3, 'IDADE'] / 12).round(1)   \n",
    "    return df\n",
    "\n",
    "def tratar_sexo(df):\n",
    "    \"\"\"Trata SEXO usando nullable integer para lidar com NaNs\"\"\"\n",
    "    if 'SEXO' not in df.columns:\n",
    "        return df\n",
    "    print(\"Tratando coluna SEXO...\")\n",
    "    # Usa Int64 nullable para lidar melhor com NaNs (melhor da V1)\n",
    "    df['SEXO'] = pd.to_numeric(df['SEXO'], errors='coerce').astype('Int64')\n",
    "    return df\n",
    "\n",
    "def tratar_datas(df):\n",
    "    \"\"\"Converte colunas de data para datetime\"\"\"\n",
    "    colunas_datas = ['DT_INTER', 'DT_SAIDA', 'NASC']\n",
    "    colunas_existentes = [col for col in colunas_datas if col in df.columns]\n",
    "    \n",
    "    for col in colunas_existentes:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y%m%d')\n",
    "    return df\n",
    "\n",
    "def tratar_valores(df):\n",
    "    \"\"\"Converte colunas de valores monetários para float\"\"\"\n",
    "    colunas_valores = ['VAL_SH', 'VAL_SP', 'VAL_TOT', 'VAL_UTI']\n",
    "    \n",
    "    for col in colunas_valores:\n",
    "        if col in df.columns:\n",
    "            print(f\"Tratando coluna {col}...\")\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(float)\n",
    "    return df\n",
    "\n",
    "def tratar_inteiros(df):\n",
    "    \"\"\"Converte colunas inteiras para int\"\"\"\n",
    "    colunas_inteiras = ['UTI_MES_TO', 'UTI_INT_TO', 'DIAR_ACOM', 'QT_DIARIAS', 'DIAS_PERM']\n",
    "    \n",
    "    for col in colunas_inteiras:\n",
    "        if col in df.columns:\n",
    "            print(f\"Tratando coluna {col}...\")\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "# Função para aplicar tratamentos sequencialmente\n",
    "def aplicar_tratamentos(df, tratamentos):\n",
    "    \"\"\"Aplica lista de funções de tratamento ao DataFrame\"\"\"\n",
    "    for func in tratamentos:\n",
    "        df = func(df)\n",
    "    return df\n",
    "\n",
    "# Processamento em chunks para arquivos grandes\n",
    "def processar_chunks(parquet_entrada, parquet_saida, tratamentos, batch_size=500_000):\n",
    "    \"\"\"Processa arquivo em chunks para economia de memória\"\"\"\n",
    "    print(f\"Processando em chunks de {batch_size:,} registros...\")\n",
    "    \n",
    "    parquet_file = pq.ParquetFile(parquet_entrada)\n",
    "    total_registros = 0\n",
    "    writer = None\n",
    "    schema_unificado = None\n",
    "    \n",
    "    try:\n",
    "        for i, batch in enumerate(parquet_file.iter_batches(batch_size=batch_size)):\n",
    "            df_chunk = batch.to_pandas()\n",
    "            print(f\"Processando chunk {i+1} ({len(df_chunk):,} registros)...\")\n",
    "\n",
    "            df_chunk = aplicar_tratamentos(df_chunk, tratamentos)\n",
    "\n",
    "            table = pa.Table.from_pandas(df_chunk, preserve_index=False)\n",
    "            \n",
    "            if writer is None:\n",
    "                # Usar schema do primeiro chunk como referência\n",
    "                schema_unificado = table.schema\n",
    "                writer = pq.ParquetWriter(parquet_saida, schema_unificado, compression=\"snappy\")\n",
    "            else:\n",
    "                # Garantir que o table tenha o mesmo schema do primeiro chunk\n",
    "                table = table.cast(schema_unificado)\n",
    "            \n",
    "            writer.write_table(table)\n",
    "            total_registros += len(df_chunk)\n",
    "            \n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "    \n",
    "    print(f\"Arquivo tratado salvo: {parquet_saida}\")\n",
    "    print(f\"Total de registros processados: {total_registros:,}\")\n",
    "\n",
    "# Processamento simples para arquivos menores\n",
    "def processar_simples(parquet_entrada, parquet_saida, tratamentos):\n",
    "    \"\"\"Processa arquivo inteiro na memória para melhor performance\"\"\"\n",
    "    print(\"Processando arquivo completo na memória...\")\n",
    "    \n",
    "    df = pd.read_parquet(parquet_entrada)\n",
    "    print(f\"Arquivo carregado: {len(df):,} registros\")\n",
    "    \n",
    "    for tratamento in tratamentos:\n",
    "        df = tratamento(df)\n",
    "    \n",
    "    df.to_parquet(parquet_saida, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    print(f\"Arquivo tratado salvo: {parquet_saida}\")\n",
    "    print(f\"Total de registros: {len(df):,}\")\n",
    "\n",
    "# Função principal com detecção automática do método\n",
    "def processar_tratamento(parquet_entrada, parquet_saida, limite_mb=500, batch_size=500_000):\n",
    "    \"\"\"\n",
    "    Processa tratamento dos dados com detecção automática do melhor método\n",
    "    \n",
    "    Args:\n",
    "        parquet_entrada: Caminho do arquivo de entrada\n",
    "        parquet_saida: Caminho do arquivo de saída\n",
    "        limite_mb: Limite em MB para escolha do método (padrão: 500MB)\n",
    "        batch_size: Tamanho dos chunks para processamento em lotes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lista de tratamentos\n",
    "    tratamentos = [\n",
    "        tratar_num_filhos,\n",
    "        tratar_instrucao,\n",
    "        tratar_cids,\n",
    "        tratar_idade,\n",
    "        tratar_sexo,\n",
    "        tratar_datas,\n",
    "        tratar_valores,\n",
    "        tratar_inteiros\n",
    "    ]\n",
    "    \n",
    "    # Verificar se arquivo existe\n",
    "    if not os.path.exists(parquet_entrada):\n",
    "        print(f\"ERRO: Arquivo {parquet_entrada} não encontrado!\")\n",
    "        return\n",
    "    \n",
    "    # Detectar tamanho do arquivo\n",
    "    tamanho_mb = os.path.getsize(parquet_entrada) / (1024 * 1024)\n",
    "    print(f\"Tamanho do arquivo: {tamanho_mb:.1f} MB\")\n",
    "    \n",
    "    # Criar pasta de saída se necessário\n",
    "    os.makedirs(os.path.dirname(parquet_saida), exist_ok=True)\n",
    "    \n",
    "    # Escolher método baseado no tamanho\n",
    "    if tamanho_mb > limite_mb:\n",
    "        print(f\"Arquivo grande detectado (>{limite_mb}MB) - usando processamento em chunks\")\n",
    "        processar_chunks(parquet_entrada, parquet_saida, tratamentos, batch_size)\n",
    "    else:\n",
    "        print(f\"Arquivo pequeno detectado (<={limite_mb}MB) - processamento na memória\")\n",
    "        processar_simples(parquet_entrada, parquet_saida, tratamentos)\n",
    "    \n",
    "    print(\"\\nTratamento concluído com sucesso!\")\n",
    "    \n",
    "    # Informações finais do arquivo\n",
    "    tamanho_final_mb = os.path.getsize(parquet_saida) / (1024 * 1024)\n",
    "    print(f\"Arquivo final: {tamanho_final_mb:.1f} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Caminhos (ajustados para estrutura correta)\n",
    "    entrada = \"../../banco/parquet_unificado/sih_rs.parquet\"\n",
    "    saida = \"../../banco/parquet_unificado/sih_rs_tratado.parquet\"\n",
    "        processar_tratamento(entrada, saida, limite_mb=500, batch_size=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e53b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca39ac-46fd-446d-8fe0-1f4b4188e38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe2c74-e521-46b8-9585-9fb3270c512a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

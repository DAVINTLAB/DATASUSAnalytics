{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e4af8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRÉ-TRATAMENTO DE DADOS - VERSÃO FINAL\n",
      "==================================================\n",
      "Entrada: ../../banco/parquet_unificado/sih_rs.parquet\n",
      "Saída: ../../banco/parquet_unificado/sih_rs_tratado.parquet\n",
      "Arquivo entrada: 792.4 MB\n",
      "Usando processamento em chunks...\n",
      "Processando em chunks de 500,000 registros...\n",
      "Processando chunk 10...\n",
      "Processando chunk 20...\n",
      "Processando chunk 30...\n",
      "Processando chunk 40...\n",
      "Concatenando chunks...\n",
      "Contraindo dados por N_AIH...\n",
      "Registros: 22,183,490 → 11,022,199 (50.3% redução)\n",
      "Salvando arquivo...\n",
      "\n",
      "==================================================\n",
      "TRATAMENTO CONCLUÍDO!\n",
      "==================================================\n",
      "Registros finais: 11,022,199\n",
      "Tamanho final: 353.6 MB\n",
      "Tempo: 362.3s (6.0 min)\n",
      "Próximo: executar bd.ipynb\n",
      "\n",
      "SUCESSO! 11,022,199 registros processados.\n",
      "Finalizado em 2025-06-06 17:12:49.626376\n"
     ]
    }
   ],
   "source": [
    "# PRÉ-TRATAMENTO - VERSÃO FINAL OTIMIZADA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurações\n",
    "ENTRADA = \"../../banco/parquet_unificado/sih_rs.parquet\"\n",
    "SAIDA = \"../../banco/parquet_unificado/sih_rs_tratado.parquet\"\n",
    "BACKUP_DIR = \"../../banco/backups\"\n",
    "LIMITE_MB = 500\n",
    "BATCH_SIZE = 500_000\n",
    "\n",
    "print(\"PRÉ-TRATAMENTO DE DADOS - VERSÃO FINAL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def verificar_memoria():\n",
    "   memory = psutil.virtual_memory()\n",
    "   if memory.percent > 85:\n",
    "       gc.collect()\n",
    "       time.sleep(1)\n",
    "   return memory.percent\n",
    "\n",
    "def tratar_num_filhos(df):\n",
    "   if 'NUM_FILHOS' in df.columns:\n",
    "       df['NUM_FILHOS'] = pd.to_numeric(df['NUM_FILHOS'], errors='coerce').fillna(0).astype(int)\n",
    "   return df\n",
    "\n",
    "def tratar_instrucao(df):\n",
    "   if 'INSTRU' not in df.columns:\n",
    "       return df\n",
    "   df['INSTRU'] = df['INSTRU'].astype(str).str.zfill(2)\n",
    "   df['INSTRU'] = df['INSTRU'].replace(['00', 'nan'], '0')\n",
    "   return df\n",
    "\n",
    "def padronizar_cids(df, colunas_cid):\n",
    "   colunas_existentes = [col for col in colunas_cid if col in df.columns]\n",
    "   if not colunas_existentes:\n",
    "       return df\n",
    "   \n",
    "   for col in colunas_existentes:\n",
    "       df[col] = df[col].astype(str).str.upper().str.strip()\n",
    "       mask = df[col].isin(['0000', 'NAN', ''])\n",
    "       df.loc[mask, col] = np.nan\n",
    "   return df\n",
    "\n",
    "def tratar_cids(df):\n",
    "   colunas_cid = [\n",
    "       'DIAG_PRINC', 'DIAG_SECUN', 'CID_NOTIF', 'CID_ASSO', 'CID_MORTE'\n",
    "   ] + [f'DIAGSEC{i}' for i in range(1, 10)]\n",
    "   return padronizar_cids(df, colunas_cid)\n",
    "\n",
    "def tratar_idade(df):\n",
    "   if 'IDADE' in df.columns:\n",
    "       df['IDADE'] = pd.to_numeric(df['IDADE'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "   if 'COD_IDADE' in df.columns:\n",
    "       df['COD_IDADE'] = pd.to_numeric(df['COD_IDADE'], errors='coerce').fillna(0).astype(int)\n",
    "       \n",
    "       df.loc[df['COD_IDADE'] == 1, 'IDADE'] = 0  \n",
    "       df.loc[df['COD_IDADE'] == 2, 'IDADE'] = (df.loc[df['COD_IDADE'] == 2, 'IDADE'] / 365).round(1) \n",
    "       df.loc[df['COD_IDADE'] == 3, 'IDADE'] = (df.loc[df['COD_IDADE'] == 3, 'IDADE'] / 12).round(1)   \n",
    "       \n",
    "       df = df.drop('COD_IDADE', axis=1)\n",
    "       \n",
    "   return df\n",
    "\n",
    "def tratar_sexo(df):\n",
    "   if 'SEXO' not in df.columns:\n",
    "       return df\n",
    "   df['SEXO'] = pd.to_numeric(df['SEXO'], errors='coerce').astype('Int64')\n",
    "   return df\n",
    "\n",
    "def tratar_datas(df):\n",
    "   colunas_datas = ['DT_INTER', 'DT_SAIDA', 'NASC']\n",
    "   colunas_existentes = [col for col in colunas_datas if col in df.columns]\n",
    "   \n",
    "   for col in colunas_existentes:\n",
    "       df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y%m%d')\n",
    "   \n",
    "   return df\n",
    "\n",
    "def tratar_valores(df):\n",
    "   colunas_valores = ['VAL_SH', 'VAL_SP', 'VAL_TOT', 'VAL_UTI']\n",
    "   \n",
    "   for col in colunas_valores:\n",
    "       if col in df.columns:\n",
    "           df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(float)\n",
    "   return df\n",
    "\n",
    "def tratar_inteiros(df):\n",
    "   colunas_inteiras = ['UTI_MES_TO', 'UTI_INT_TO', 'DIAR_ACOM', 'QT_DIARIAS', 'DIAS_PERM']\n",
    "   \n",
    "   for col in colunas_inteiras:\n",
    "       if col in df.columns:\n",
    "           df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "   return df\n",
    "\n",
    "def contrair_naih(df):\n",
    "   print(\"Contraindo dados por N_AIH...\")\n",
    "   registros_antes = len(df)\n",
    "   aihs_unicas_antes = df['N_AIH'].nunique()\n",
    "   \n",
    "   if registros_antes == aihs_unicas_antes:\n",
    "       print(\"Não há duplicatas por N_AIH.\")\n",
    "       return df\n",
    "   \n",
    "   colunas_soma = ['VAL_SH', 'VAL_SP', 'VAL_TOT', 'VAL_UTI', 'QT_DIARIAS', 'DIAS_PERM']\n",
    "   colunas_media = ['UTI_MES_TO', 'UTI_INT_TO', 'DIAR_ACOM', 'IDADE']\n",
    "   \n",
    "   agg_dict = {}\n",
    "   \n",
    "   for col in df.columns:\n",
    "       if col == 'N_AIH':\n",
    "           continue\n",
    "       elif col in colunas_soma:\n",
    "           agg_dict[col] = 'sum'\n",
    "       elif col in colunas_media:\n",
    "           agg_dict[col] = 'mean'\n",
    "       else:\n",
    "           agg_dict[col] = 'first'\n",
    "   \n",
    "   df_contraido = df.groupby('N_AIH').agg(agg_dict).reset_index()\n",
    "   \n",
    "   for col in colunas_media:\n",
    "       if col in df_contraido.columns:\n",
    "           df_contraido[col] = df_contraido[col].round(1)\n",
    "   \n",
    "   print(f\"Registros: {registros_antes:,} → {len(df_contraido):,} ({((registros_antes - len(df_contraido)) / registros_antes * 100):.1f}% redução)\")\n",
    "   \n",
    "   return df_contraido\n",
    "\n",
    "def aplicar_tratamentos(df, tratamentos):\n",
    "   for func in tratamentos:\n",
    "       df = func(df)\n",
    "   return df\n",
    "\n",
    "def processar_chunks(parquet_entrada, parquet_saida, tratamentos, batch_size=500_000):\n",
    "   print(f\"Processando em chunks de {batch_size:,} registros...\")\n",
    "   \n",
    "   parquet_file = pq.ParquetFile(parquet_entrada)\n",
    "   chunks_processados = []\n",
    "   \n",
    "   batch_num = 0\n",
    "   for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "       batch_num += 1\n",
    "       df_chunk = batch.to_pandas()\n",
    "       \n",
    "       if batch_num % 10 == 0:\n",
    "           print(f\"Processando chunk {batch_num}...\")\n",
    "       \n",
    "       tratamentos_chunk = [t for t in tratamentos if t.__name__ != 'contrair_naih']\n",
    "       df_chunk = aplicar_tratamentos(df_chunk, tratamentos_chunk)\n",
    "       chunks_processados.append(df_chunk)\n",
    "       \n",
    "   print(\"Concatenando chunks...\")\n",
    "   df_completo = pd.concat(chunks_processados, ignore_index=True)\n",
    "   \n",
    "   if any(t.__name__ == 'contrair_naih' for t in tratamentos):\n",
    "       df_completo = contrair_naih(df_completo)\n",
    "\n",
    "   print(\"Salvando arquivo...\")\n",
    "   table = pa.Table.from_pandas(df_completo, preserve_index=False)\n",
    "   pq.write_table(table, parquet_saida, compression=\"snappy\")\n",
    "   \n",
    "   return len(df_completo)\n",
    "\n",
    "def processar_simples(parquet_entrada, parquet_saida, tratamentos):\n",
    "   print(\"Processando arquivo na memória...\")\n",
    "   \n",
    "   df = pd.read_parquet(parquet_entrada)\n",
    "   print(f\"Carregado: {len(df):,} registros\")\n",
    "   \n",
    "   for tratamento in tratamentos:\n",
    "       df = tratamento(df)\n",
    "   \n",
    "   print(\"Salvando arquivo...\")\n",
    "   df.to_parquet(parquet_saida, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "   \n",
    "   return len(df)\n",
    "\n",
    "def processar_tratamento(parquet_entrada, parquet_saida, limite_mb=500, batch_size=500_000):\n",
    "   tratamentos = [\n",
    "       tratar_num_filhos,\n",
    "       tratar_instrucao,\n",
    "       tratar_cids,\n",
    "       tratar_idade,\n",
    "       tratar_sexo,\n",
    "       tratar_datas,\n",
    "       tratar_valores,\n",
    "       tratar_inteiros,\n",
    "       contrair_naih\n",
    "   ]\n",
    "   \n",
    "   if not os.path.exists(parquet_entrada):\n",
    "       raise FileNotFoundError(f\"Arquivo {parquet_entrada} não encontrado!\")\n",
    "   \n",
    "   tamanho_mb = os.path.getsize(parquet_entrada) / (1024 * 1024)\n",
    "   print(f\"Arquivo entrada: {tamanho_mb:.1f} MB\")\n",
    "   \n",
    "   Path(parquet_saida).parent.mkdir(parents=True, exist_ok=True)\n",
    "   \n",
    "   if os.path.exists(parquet_saida):\n",
    "       timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "       backup_path = f\"{BACKUP_DIR}/sih_rs_tratado_backup_{timestamp}.parquet\"\n",
    "       Path(BACKUP_DIR).mkdir(parents=True, exist_ok=True)\n",
    "       os.rename(parquet_saida, backup_path)\n",
    "       print(f\"Backup criado: {backup_path}\")\n",
    "   \n",
    "   inicio = time.time()\n",
    "   \n",
    "   if tamanho_mb > limite_mb:\n",
    "       print(\"Usando processamento em chunks...\")\n",
    "       registros_finais = processar_chunks(parquet_entrada, parquet_saida, tratamentos, batch_size)\n",
    "   else:\n",
    "       registros_finais = processar_simples(parquet_entrada, parquet_saida, tratamentos)\n",
    "   \n",
    "   tempo_total = time.time() - inicio\n",
    "   tamanho_final_mb = os.path.getsize(parquet_saida) / (1024 * 1024)\n",
    "   \n",
    "   print(\"\\n\" + \"=\"*50)\n",
    "   print(\"TRATAMENTO CONCLUÍDO!\")\n",
    "   print(\"=\"*50)\n",
    "   print(f\"Registros finais: {registros_finais:,}\")\n",
    "   print(f\"Tamanho final: {tamanho_final_mb:.1f} MB\")\n",
    "   print(f\"Tempo: {tempo_total:.1f}s ({tempo_total/60:.1f} min)\")\n",
    "   print(f\"Próximo: executar bd.ipynb\")\n",
    "   \n",
    "   return registros_finais\n",
    "\n",
    "# Verificar arquivo de entrada\n",
    "if not os.path.exists(ENTRADA):\n",
    "   raise FileNotFoundError(f\"Arquivo não encontrado: {ENTRADA}\")\n",
    "\n",
    "print(f\"Entrada: {ENTRADA}\")\n",
    "print(f\"Saída: {SAIDA}\")\n",
    "\n",
    "# Executar processamento\n",
    "try:\n",
    "   resultado = processar_tratamento(ENTRADA, SAIDA, LIMITE_MB, BATCH_SIZE)\n",
    "   print(f\"\\nSUCESSO! {resultado:,} registros processados.\")\n",
    "   \n",
    "except Exception as e:\n",
    "   print(f\"\\nERRO: {e}\")\n",
    "   raise\n",
    "\n",
    "finally:\n",
    "   gc.collect()\n",
    "   print(f\"Finalizado em {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e53b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca39ac-46fd-446d-8fe0-1f4b4188e38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe2c74-e521-46b8-9585-9fb3270c512a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
